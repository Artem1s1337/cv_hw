# -*- coding: utf-8 -*-
"""hand_keypoints_3d.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qw2tfhOkHB8YpeZpalFGj7A7HLN5oZeo

# 3D Hand Keypoints Regression

**–ó–∞–¥–∞—á–∞**: –†–µ–≥—Ä–µ—Å—Å–∏—è 3D –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç (xyz) –∫–ª—é—á–µ–≤—ã—Ö —Ç–æ—á–µ–∫ —Ä—É–∫–∏.

## üéØ –¶–µ–ª—å: 3D MPJPE ‚â§ 20 mm

–ó–∞–ø–æ–ª–Ω–∏—Ç–µ –ø—Ä–æ–ø—É—Å–∫–∏ (TODO) –∏ –¥–æ–±–µ–π—Ç–µ—Å—å –∫–∞—á–µ—Å—Ç–≤–∞ ‚â§ 20 mm –Ω–∞ —Ç–µ—Å—Ç–µ.

## –ü–æ–¥—Ö–æ–¥
- –ü—Ä—è–º–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è xyz –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç
- –í—Ö–æ–¥: –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ 224√ó224
- –í—ã—Ö–æ–¥: 63 –∑–Ω–∞—á–µ–Ω–∏—è (21 —Ç–æ—á–∫–∞ √ó 3 –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã)
"""

!pip install pytorch-lightning

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, models
import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping
import numpy as np
from PIL import Image
import os
import json
import zipfile
from tqdm import tqdm
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

torch.manual_seed(42)
np.random.seed(42)
pl.seed_everything(42)

"""## –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ FreiHAND

"""

DATA_DIR = "../data/freihand"
EVAL_DIR = os.path.join(DATA_DIR, "evaluation")

if not os.path.exists(EVAL_DIR):
    os.makedirs(DATA_DIR, exist_ok=True)
    EVAL_ZIP = os.path.join(DATA_DIR, "FreiHAND_pub_v2_eval.zip")
    print("–°–∫–∞—á–∏–≤–∞–µ–º FreiHAND evaluation set...")
    !wget -q --show-progress -O {EVAL_ZIP} https://lmb.informatik.uni-freiburg.de/data/freihand/FreiHAND_pub_v2_eval.zip
    print("–†–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º –∞—Ä—Ö–∏–≤...")
    with zipfile.ZipFile(EVAL_ZIP, 'r') as zip_ref:
        zip_ref.extractall(DATA_DIR)
    os.remove(EVAL_ZIP)
    print("–ì–æ—Ç–æ–≤–æ!")
else:
    print(f"–î–∞—Ç–∞—Å–µ—Ç —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –≤ {EVAL_DIR}")

# –ó–∞–≥—Ä—É–∂–∞–µ–º 3D –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã
with open(os.path.join(DATA_DIR, "evaluation_xyz.json"), "r") as f:
    xyz_coordinates = json.load(f)

xyz_coordinates = np.array(xyz_coordinates)  # (N, 21, 3) –≤ –º–µ—Ç—Ä–∞—Ö

print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(xyz_coordinates)}")
print(f"–§–æ—Ä–º–∞ xyz –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç: {xyz_coordinates.shape}")
print(f"\n–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç (–≤ –º–µ—Ç—Ä–∞—Ö):")
print(f"  X: [{xyz_coordinates[:,:,0].min():.3f}, {xyz_coordinates[:,:,0].max():.3f}]")
print(f"  Y: [{xyz_coordinates[:,:,1].min():.3f}, {xyz_coordinates[:,:,1].max():.3f}]")
print(f"  Z: [{xyz_coordinates[:,:,2].min():.3f}, {xyz_coordinates[:,:,2].max():.3f}]")

"""## –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è 3D –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç

–î–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã:
- –î–µ–ª–∞–µ–º –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã **root-relative** (–æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ wrist)
- –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º –≤ —É–¥–æ–±–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω

"""

# Root-relative –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã (–æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ wrist - —Ç–æ—á–∫–∞ 0)
xyz_root_relative = xyz_coordinates - xyz_coordinates[:, 0:1, :]  # –í—ã—á–∏—Ç–∞–µ–º –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã wrist

# –ú–∞—Å—à—Ç–∞–±–∏—Ä—É—é—â–∏–π –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç (—á—Ç–æ–±—ã –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –±—ã–ª–∏ –≤ —É–¥–æ–±–Ω–æ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ)
XYZ_SCALE = 0.2  # –ü—Ä–∏–º–µ—Ä–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Ä—É–∫–∏ ~20—Å–º

# –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã
xyz_normalized = xyz_root_relative / XYZ_SCALE

print(f"Root-relative –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ):")
print(f"  X: [{xyz_normalized[:,:,0].min():.2f}, {xyz_normalized[:,:,0].max():.2f}]")
print(f"  Y: [{xyz_normalized[:,:,1].min():.2f}, {xyz_normalized[:,:,1].max():.2f}]")
print(f"  Z: [{xyz_normalized[:,:,2].min():.2f}, {xyz_normalized[:,:,2].max():.2f}]")

"""## Dataset –∫–ª–∞—Å—Å

"""

class FreiHand3DDataset(Dataset):
    """Dataset –¥–ª—è 3D —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –∫–ª—é—á–µ–≤—ã—Ö —Ç–æ—á–µ–∫ —Ä—É–∫–∏."""

    def __init__(self, data_dir, xyz_normalized, xyz_original, indices=None, img_size=224):
        self.data_dir = data_dir
        self.img_size = img_size
        self.indices = indices if indices is not None else list(range(len(xyz_normalized)))
        self.xyz_normalized = xyz_normalized
        self.xyz_original = xyz_original

        self.transform = transforms.Compose([
            transforms.Resize((img_size, img_size)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])

    def __len__(self):
        return len(self.indices)

    def __getitem__(self, idx):
        real_idx = self.indices[idx]

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        img_path = os.path.join(self.data_dir, "evaluation", "rgb", f"{real_idx:08d}.jpg")
        image = Image.open(img_path).convert('RGB')
        image = self.transform(image)

        # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ xyz –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã (–¥–ª—è –æ–±—É—á–µ–Ω–∏—è)
        xyz_norm = torch.tensor(self.xyz_normalized[real_idx], dtype=torch.float32).flatten()  # (63,)

        # –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ xyz –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –≤ –º–µ—Ç—Ä–∞—Ö (–¥–ª—è –º–µ—Ç—Ä–∏–∫)
        xyz_orig = torch.tensor(self.xyz_original[real_idx], dtype=torch.float32)  # (21, 3)

        return image, xyz_norm, xyz_orig

IMG_SIZE = 224
BATCH_SIZE = 32

NUM_SAMPLES = len(xyz_coordinates)
indices = np.random.permutation(NUM_SAMPLES)

train_size = int(0.7 * NUM_SAMPLES)
val_size = int(0.15 * NUM_SAMPLES)

train_indices = indices[:train_size].tolist()
val_indices = indices[train_size:train_size + val_size].tolist()
test_indices = indices[train_size + val_size:].tolist()

train_dataset = FreiHand3DDataset(DATA_DIR, xyz_normalized, xyz_root_relative, train_indices, IMG_SIZE)
val_dataset = FreiHand3DDataset(DATA_DIR, xyz_normalized, xyz_root_relative, val_indices, IMG_SIZE)
test_dataset = FreiHand3DDataset(DATA_DIR, xyz_normalized, xyz_root_relative, test_indices, IMG_SIZE)

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)

print(f"Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}")

"""## –ú–æ–¥–µ–ª—å: ResNet18 –¥–ª—è 3D —Ä–µ–≥—Ä–µ—Å—Å–∏–∏

"""

class Keypoint3DRegressor(nn.Module):
    """ResNet18 –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ 3D –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –≥–æ–ª–æ–≤–æ–π"""

    def __init__(self, num_keypoints=21, pretrained=True):
        super().__init__()

        self.backbone = models.resnet18(weights='IMAGENET1K_V1' if pretrained else None)
        num_features = self.backbone.fc.in_features

        # –ó–∞–º–µ–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–π —Å–ª–æ–π –Ω–∞ —É–ª—É—á—à–µ–Ω–Ω—É—é –≥–æ–ª–æ–≤—É
        self.backbone.fc = nn.Identity()  # –£–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ä—É—é –≥–æ–ª–æ–≤—É

        # –£–ª—É—á—à–µ–Ω–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–∞—è –≥–æ–ª–æ–≤–∞
        self.regression_head = nn.Sequential(
            # –ü–µ—Ä–≤—ã–π –±–ª–æ–∫
            nn.Linear(num_features, 1024),
            nn.BatchNorm1d(1024),
            nn.ReLU(inplace=True),
            nn.Dropout(0.4),

            # –í—Ç–æ—Ä–æ–π –±–ª–æ–∫
            nn.Linear(1024, 512),
            nn.BatchNorm1d(512),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),

            # –¢—Ä–µ—Ç–∏–π –±–ª–æ–∫
            nn.Linear(512, 256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.2),

            # –§–∏–Ω–∞–ª—å–Ω—ã–π —Å–ª–æ–π
            nn.Linear(256, num_keypoints * 3)
        )

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ –≥–æ–ª–æ–≤—ã
        self._init_weights()

    def _init_weights(self):
        for m in self.regression_head.modules():
            if isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm1d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)

    def forward(self, x):
        features = self.backbone(x)
        return self.regression_head(features)

"""## PyTorch Lightning –º–æ–¥—É–ª—å

–ú–µ—Ç—Ä–∏–∫–∞: **3D MPJPE** –≤ –º–∏–ª–ª–∏–º–µ—Ç—Ä–∞—Ö

"""

class Keypoint3DModule(pl.LightningModule):
    def __init__(self, num_keypoints=21, learning_rate=1e-4, xyz_scale=0.2):
        super().__init__()
        self.save_hyperparameters()

        self.model = Keypoint3DRegressor(num_keypoints=num_keypoints)
        self.num_keypoints = num_keypoints
        self.learning_rate = learning_rate
        self.xyz_scale = xyz_scale

        # Smooth L1 Loss –¥–ª—è –±–æ–ª–µ–µ —É—Å—Ç–æ–π—á–∏–≤–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
        self.loss_fn = nn.SmoothL1Loss(beta=0.1)

    def forward(self, x):
        return self.model(x)

    def compute_mpjpe_3d(self, pred, target):
        """–í—ã—á–∏—Å–ª—è–µ—Ç 3D MPJPE –≤ –º–∏–ª–ª–∏–º–µ—Ç—Ä–∞—Ö."""
        pred = pred.view(-1, self.num_keypoints, 3) * self.xyz_scale
        distances = torch.sqrt(((pred - target) ** 2).sum(dim=-1))
        mpjpe = distances.mean() * 1000
        return mpjpe

    # –û–±–Ω–æ–≤–ª—è–µ–º training_step –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è Smooth L1 Loss
    def training_step(self, batch, batch_idx):
        images, xyz_norm, xyz_orig = batch
        pred = self.forward(images)

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º Smooth L1 Loss –≤–º–µ—Å—Ç–æ MSE
        loss = self.loss_fn(pred, xyz_norm)
        mpjpe = self.compute_mpjpe_3d(pred, xyz_orig)

        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)
        self.log('train_mpjpe', mpjpe, on_step=False, on_epoch=True)
        return loss

    def validation_step(self, batch, batch_idx):
        images, xyz_norm, xyz_orig = batch
        pred = self.forward(images)

        # TODO: –í—ã—á–∏—Å–ª–∏—Ç–µ loss (–∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ training_step)
        loss = F.mse_loss(pred, xyz_norm)  # <-- –∑–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –∫–æ–¥

        mpjpe = self.compute_mpjpe_3d(pred, xyz_orig)

        self.log('val_loss', loss, on_epoch=True, prog_bar=True)
        self.log('val_mpjpe', mpjpe, on_epoch=True, prog_bar=True)
        return loss

    def test_step(self, batch, batch_idx):
        images, xyz_norm, xyz_orig = batch
        pred = self.forward(images)

        loss = F.mse_loss(pred, xyz_norm)
        mpjpe = self.compute_mpjpe_3d(pred, xyz_orig)

        self.log('test_loss', loss)
        self.log('test_mpjpe', mpjpe)
        return loss

    def configure_optimizers(self):
        # TODO: –°–æ–∑–¥–∞–π—Ç–µ optimizer –∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) scheduler
        # Hint: AdamW —Å weight_decay=1e-4 —Ä–∞–±–æ—Ç–∞–µ—Ç —Ö–æ—Ä–æ—à–æ

        # –†–µ–∞–ª–∏–∑–∞—Ü–∏—è:
        optimizer = torch.optim.AdamW(
            self.parameters(),
            lr=self.learning_rate,
            weight_decay=1e-4
        )

        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer,
            mode='min',
            factor=0.5,
            patience=5,
        )

        return {
            "optimizer": optimizer,
            "lr_scheduler": {
                "scheduler": scheduler,
                "monitor": "val_loss",
                "interval": "epoch",
                "frequency": 1,
            },
        }

NUM_EPOCHS = 100  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö

# –£–ª—É—á—à–µ–Ω–Ω–∞—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –¥–ª—è –æ–±—É—á–∞—é—â–µ–≥–æ –Ω–∞–±–æ—Ä–∞
train_transform = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),
    transforms.RandomRotation(degrees=15),
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# –û–±–Ω–æ–≤–ª—è–µ–º transform –≤ train_dataset
train_dataset.transform = train_transform

model = Keypoint3DModule(num_keypoints=21, learning_rate=1e-4, xyz_scale=XYZ_SCALE)

# –ò—Å–ø–æ–ª—å–∑—É–µ–º gradient clipping –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è
trainer = pl.Trainer(
    max_epochs=NUM_EPOCHS,
    accelerator='auto',
    devices=1,
    gradient_clip_val=1.0,  # –î–æ–±–∞–≤–ª—è–µ–º gradient clipping
    accumulate_grad_batches=2,  # –ù–∞–∫–æ–ø–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
    callbacks=[
        ModelCheckpoint(
            monitor='val_mpjpe',
            mode='min',
            save_top_k=3,
            filename='best-{epoch:02d}-{val_mpjpe:.2f}'
        ),
        EarlyStopping(
            monitor='val_mpjpe',
            mode='min',
            patience=15,
            min_delta=0.1
        )
    ],
    enable_progress_bar=True,
    log_every_n_steps=10
)

trainer.fit(model, train_loader, val_loader)

test_results = trainer.test(model, test_loader)
print(f"\n–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —Ç–µ—Å—Ç–µ:")
print(f"  3D MPJPE: {test_results[0]['test_mpjpe']:.2f} mm")

"""## –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è 3D –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π

"""

SKELETON = [
    [0, 1], [1, 2], [2, 3], [3, 4],      # Thumb
    [0, 5], [5, 6], [6, 7], [7, 8],      # Index
    [0, 9], [9, 10], [10, 11], [11, 12], # Middle
    [0, 13], [13, 14], [14, 15], [15, 16], # Ring
    [0, 17], [17, 18], [18, 19], [19, 20]  # Pinky
]

FINGER_COLORS = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7']

def denormalize_image(tensor):
    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)
    return torch.clamp(tensor * std + mean, 0, 1).permute(1, 2, 0).numpy()

def visualize_3d_predictions(model, dataset, num_samples=4):
    model.eval()
    device = next(model.parameters()).device

    fig = plt.figure(figsize=(12, 4*num_samples))
    indices = np.random.choice(len(dataset), num_samples, replace=False)

    with torch.no_grad():
        for i, idx in enumerate(indices):
            image, xyz_norm, xyz_orig = dataset[idx]

            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            pred = model(image.unsqueeze(0).to(device)).cpu().squeeze()
            pred_xyz = pred.view(21, 3).numpy() * XYZ_SCALE * 1000  # –≤ –º–º
            gt_xyz = xyz_orig.numpy() * 1000  # –≤ –º–º

            # –û—à–∏–±–∫–∞
            error = np.sqrt(((pred_xyz - gt_xyz) ** 2).sum(axis=1)).mean()

            # 1. –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            ax1 = fig.add_subplot(num_samples, 2, i*2 + 1)
            ax1.imshow(denormalize_image(image))
            ax1.set_title(f'Sample {idx}')
            ax1.axis('off')

            # 2. 3D —Å–∫–µ–ª–µ—Ç (GT –∑–µ–ª—ë–Ω—ã–π, Pred –∫—Ä–∞—Å–Ω—ã–π)
            ax2 = fig.add_subplot(num_samples, 2, i*2 + 2, projection='3d')

            # GT - –∑–µ–ª—ë–Ω—ã–π
            for edge in SKELETON:
                ax2.plot3D([gt_xyz[edge[0], 0], gt_xyz[edge[1], 0]],
                          [gt_xyz[edge[0], 2], gt_xyz[edge[1], 2]],  # Z –∫–∞–∫ –≥–ª—É–±–∏–Ω–∞
                          [gt_xyz[edge[0], 1], gt_xyz[edge[1], 1]],  # Y –∫–∞–∫ –≤—ã—Å–æ—Ç–∞ (–∏–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º)
                          'g-', linewidth=2, alpha=0.7)
            ax2.scatter(gt_xyz[:, 0], gt_xyz[:, 2], gt_xyz[:, 1], c='green', s=30, label='GT')

            # Pred - –∫—Ä–∞—Å–Ω—ã–π
            for edge in SKELETON:
                ax2.plot3D([pred_xyz[edge[0], 0], pred_xyz[edge[1], 0]],
                          [pred_xyz[edge[0], 2], pred_xyz[edge[1], 2]],
                          [pred_xyz[edge[0], 1], pred_xyz[edge[1], 1]],
                          'r-', linewidth=2, alpha=0.7)
            ax2.scatter(pred_xyz[:, 0], pred_xyz[:, 2], pred_xyz[:, 1], c='red', s=30, label='Pred')

            ax2.set_xlabel('X (mm)')
            ax2.set_ylabel('Z (mm)')
            ax2.set_zlabel('Y (mm)')
            ax2.set_title(f'3D MPJPE: {error:.1f} mm')
            ax2.legend()
            ax2.invert_zaxis()  # –ò–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º Y (–≤–≤–µ—Ä—Ö - –º–µ–Ω—å—à–µ)

    plt.tight_layout()
    plt.show()

visualize_3d_predictions(model, test_dataset, num_samples=4)

"""## –ê–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫ –ø–æ —Ç–æ—á–∫–∞–º

"""

KEYPOINT_NAMES = [
    'Wrist',
    'Thumb_CMC', 'Thumb_MCP', 'Thumb_IP', 'Thumb_Tip',
    'Index_MCP', 'Index_PIP', 'Index_DIP', 'Index_Tip',
    'Middle_MCP', 'Middle_PIP', 'Middle_DIP', 'Middle_Tip',
    'Ring_MCP', 'Ring_PIP', 'Ring_DIP', 'Ring_Tip',
    'Pinky_MCP', 'Pinky_PIP', 'Pinky_DIP', 'Pinky_Tip'
]

def compute_per_keypoint_error(model, dataloader):
    model.eval()
    device = next(model.parameters()).device
    all_errors = []

    with torch.no_grad():
        for images, xyz_norm, xyz_orig in tqdm(dataloader, desc="Computing errors"):
            images = images.to(device)
            pred = model(images).cpu()

            pred = pred.view(-1, 21, 3) * XYZ_SCALE  # –≤ –º–µ—Ç—Ä–∞—Ö

            # 3D –æ—à–∏–±–∫–∏ –≤ –º–º
            errors = torch.sqrt(((pred - xyz_orig) ** 2).sum(dim=-1)) * 1000
            all_errors.append(errors)

    all_errors = torch.cat(all_errors, dim=0)
    return all_errors.mean(dim=0).numpy()

per_kp_errors = compute_per_keypoint_error(model, test_loader)

fig, ax = plt.subplots(figsize=(14, 6))

colors = ['#2C3E50'] + ['#FF6B6B']*4 + ['#4ECDC4']*4 + ['#45B7D1']*4 + ['#96CEB4']*4 + ['#FFEAA7']*4

ax.bar(range(21), per_kp_errors, color=colors, edgecolor='black')
ax.set_xticks(range(21))
ax.set_xticklabels(KEYPOINT_NAMES, rotation=45, ha='right')
ax.set_ylabel('3D MPJPE (mm)')
ax.set_title('3D –æ—à–∏–±–∫–∞ –ø–æ –∫–∞–∂–¥–æ–π –∫–ª—é—á–µ–≤–æ–π —Ç–æ—á–∫–µ')
ax.axhline(y=per_kp_errors.mean(), color='red', linestyle='--', label=f'Mean: {per_kp_errors.mean():.1f} mm')
ax.legend()
ax.grid(axis='y', alpha=0.3)

plt.tight_layout()
plt.show()